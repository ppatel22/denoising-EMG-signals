{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as torch_utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import spectrogram\n",
    "from tqdm.auto import tqdm\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "# Uncomment the line below to download the dataset (when running on Google Colab)\n",
    "# !gdown 1Y42nOM606No8IXlJB2Ezizu6H0QMprHT\n",
    "# filepath = \"/content/short_data.pkl\"\n",
    "\n",
    "filepath = \"short_data.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMGsignals(Dataset):\n",
    "  def __init__(self, filename, train, split=0.5):\n",
    "      self.data = pd.read_pickle(filename)\n",
    "      self.train = train\n",
    "      self.raw = self.data[\"raw\"]\n",
    "      self.preprocess = self.data[\"preprocess\"]\n",
    "      self.n = self.data.shape[0]\n",
    "      self.split = split\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "      if self.train:\n",
    "          raw_tensor = torch.tensor(self.raw[index], dtype=torch.float32).to(device)\n",
    "          preprocess_tensor = torch.tensor(self.preprocess[index], dtype=torch.float32).to(device)\n",
    "          return raw_tensor, preprocess_tensor\n",
    "      else:\n",
    "          constant = self.split * self.n\n",
    "          raw_tensor = torch.tensor(self.raw[index + 100], dtype=torch.float32).to(device)\n",
    "          preprocess_tensor = torch.tensor(self.preprocess[index + 100], dtype=torch.float32).to(device)\n",
    "          return raw_tensor, preprocess_tensor\n",
    "\n",
    "  def __len__(self):\n",
    "      return int(self.n * self.split) if self.train else int(self.n * (1 - self.split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = EMGsignals(filepath, train=True, split=0.8)\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "val_data = EMGsignals(filepath, train=False, split=0.2)\n",
    "batch_size = 32\n",
    "val_dataloader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, dim: int, n_hidden: int):\n",
    "        # dim: the dimension of the input\n",
    "        # n_hidden: the dimension of the keys, queries, and values\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_K = nn.Linear(dim, n_hidden)  # W_K weight matrix\n",
    "        self.W_Q = nn.Linear(dim, n_hidden)  # W_Q weight matrix\n",
    "        self.W_V = nn.Linear(dim, n_hidden)  # W_V weight matrix\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "    def forward(self, x, attn_mask):\n",
    "        # x                the inputs. shape: (B x T x dim)\n",
    "        # attn_mask        an attention mask. If None, ignore. If not None, then mask[b, i, j]\n",
    "        #                  contains 1 if (in batch b) token i should attend on token j and 0\n",
    "        #                  otherwise. shape: (B x T x T)\n",
    "        # Outputs:\n",
    "        # attn_output      the output of performing self-attention on x. shape: (Batch x Num_tokens x n_hidden)\n",
    "\n",
    "        Q = self.W_Q(x)\n",
    "        K = self.W_K(x)\n",
    "        V = self.W_V(x)\n",
    "        num = torch.matmul(Q, K.transpose(-2, -1))\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            num = num.masked_fill(attn_mask == 0, -1e6)\n",
    "        \n",
    "        alpha = nn.functional.softmax(\n",
    "            num / torch.sqrt(torch.tensor(self.n_hidden, dtype=torch.float)), dim=-1\n",
    "        )\n",
    "        \n",
    "        attn_output = torch.matmul(alpha, V)\n",
    "\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, dim: int, n_hidden: int, num_heads: int):\n",
    "        # dim: the dimension of the input\n",
    "        # n_hidden: the hidden dimenstion for the attention layer\n",
    "        # num_heads: the number of attention heads\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.n_hidden = n_hidden\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.heads = []\n",
    "        for _ in range(self.num_heads):\n",
    "            self.heads.append(AttentionHead(self.dim, self.n_hidden))\n",
    "        self.heads = nn.ModuleList(self.heads)\n",
    "\n",
    "        self.project = nn.Linear(self.num_heads * self.n_hidden, self.dim)\n",
    "\n",
    "    def forward(self, x, attn_mask):\n",
    "        # x                the inputs. shape: (B x T x dim)\n",
    "        # attn_mask        an attention mask. If None, ignore. If not None, then mask[b, i, j]\n",
    "        #                  contains 1 if (in batch b) token i should attend on token j and 0\n",
    "        #                  otherwise. shape: (B x T x T)\n",
    "        #\n",
    "        # Outputs:\n",
    "        # attn_output      the output of performing multi-headed self-attention on x.\n",
    "        #                  shape: (B x T x dim)\n",
    "\n",
    "\n",
    "        attn_output = []\n",
    "\n",
    "        for head in self.heads:\n",
    "            temp1 = head.forward(x, attn_mask)\n",
    "            attn_output.append(temp1)\n",
    "\n",
    "        attn_output = self.project(torch.cat(attn_output, dim=-1))\n",
    "\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, dim: int, n_hidden: int):\n",
    "        # dim       the dimension of the input\n",
    "        # n_hidden  the width of the linear layer\n",
    "\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, n_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(n_hidden, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor)-> torch.Tensor:\n",
    "        # x         the input. shape: (B x T x dim)\n",
    "\n",
    "        # Outputs:\n",
    "        # out       the output of the feed-forward network: (B x T x dim)\n",
    "        return self.net(x)\n",
    "\n",
    "class AttentionResidual(nn.Module):\n",
    "    def __init__(self, dim: int, attn_dim: int, mlp_dim: int, num_heads: int):\n",
    "        # dim       the dimension of the input\n",
    "        # attn_dim  the hidden dimension of the attention layer\n",
    "        # mlp_dim   the hidden layer of the FFN\n",
    "        # num_heads the number of heads in the attention layer\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadedAttention(dim, attn_dim, num_heads)\n",
    "        self.ffn = FFN(dim, mlp_dim)\n",
    "\n",
    "    def forward(self, x, attn_mask):\n",
    "        # x                the inputs. shape: (B x T x dim)\n",
    "        # attn_mask        an attention mask. If None, ignore. If not None, then mask[b, i, j]\n",
    "        #                  contains 1 if (in batch b) token i should attend on token j and 0\n",
    "        #                  otherwise. shape: (B x T x T)\n",
    "        #\n",
    "        # Outputs:\n",
    "        # attn_out         shape: (B x T x dim)\n",
    "\n",
    "        attn_out = self.attn(x=x, attn_mask=attn_mask)\n",
    "        x = attn_out + x\n",
    "        x = self.ffn(x) + x\n",
    "        return x\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_layers, attn_dim=16, mlp_dim=16):\n",
    "        # dim       the dimension of the input\n",
    "        # attn_dim  the hidden dimension of the attention layer\n",
    "        # mlp_dim   the hidden layer of the FFN\n",
    "        # num_heads the number of heads in the attention layer\n",
    "        # num_layers the number of attention layers.\n",
    "        super().__init__()\n",
    "\n",
    "        self.residuals = []\n",
    "        for _ in range(num_layers):\n",
    "            self.residuals.append(AttentionResidual(dim, attn_dim, mlp_dim, num_heads))\n",
    "        self.residuals = nn.ModuleList(self.residuals)\n",
    "\n",
    "        self.dim = dim\n",
    "        self.attn_dim = attn_dim\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x, attn_mask):\n",
    "        # x                the inputs. shape: (B x T x dim)\n",
    "        # attn_mask        an attention mask. Pass this to each of the AttentionResidual layers!\n",
    "        #                  shape: (B x T x T)\n",
    "        #\n",
    "        # Outputs:\n",
    "        # attn_output      shape: (B x T x dim)\n",
    "\n",
    "        output = x\n",
    "\n",
    "        for layer in self.residuals:\n",
    "            output = layer(output, attn_mask)\n",
    "\n",
    "        return output\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, num_attn_layers, num_linear, lin_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.transformer = TransformerLayer(input_dim, num_heads, num_attn_layers)\n",
    "        self.linear = nn.Sequential()\n",
    "\n",
    "        if num_linear < 2:\n",
    "            raise AssertionError(\"Number of linear layers must be at least 2\")\n",
    "\n",
    "        for i in range(num_linear):\n",
    "            if i == 0:\n",
    "                self.linear.add_module(f\"linear_{i}\", nn.Linear(input_dim, lin_dim))\n",
    "                self.linear.add_module(f\"relu_{i}\", nn.ReLU())\n",
    "            elif i == num_linear - 1:\n",
    "                self.linear.add_module(f\"linear_{i}\", nn.Linear(lin_dim, latent_dim))\n",
    "            else:\n",
    "                self.linear.add_module(f\"linear_{i}\", nn.Linear(lin_dim, lin_dim))\n",
    "                self.linear.add_module(f\"relu_{i}\", nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        attended = self.transformer(x, attn_mask=None)\n",
    "        output = self.linear(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, output_size, inner_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.lin1 = nn.Linear(input_size, input_size)\n",
    "\n",
    "        self.inner = nn.ModuleList()\n",
    "\n",
    "        if inner_layers < 1:\n",
    "            raise AssertionError(\"Number of inner linear layers must be at least 1\")\n",
    "\n",
    "        for i in range(inner_layers):\n",
    "            self.inner.append(nn.Linear(input_size, input_size))\n",
    "\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.nonlinear = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = self.nonlinear(x)\n",
    "\n",
    "        for inner_layer in self.inner:\n",
    "            x = inner_layer(x)\n",
    "            x = self.nonlinear(x)\n",
    "\n",
    "        x = self.linear(x)\n",
    "        x = self.nonlinear(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, overshoot_penalty):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.overshoot_penalty = overshoot_penalty\n",
    "\n",
    "    def forward(self, guess, truth):\n",
    "        assert guess.shape == truth.shape, \"Input shapes do not match.\"\n",
    "\n",
    "        error = torch.abs(guess - truth)\n",
    "        overshoot_penalty = torch.where(truth < guess, self.overshoot_penalty * error, error)\n",
    "        mean_error = overshoot_penalty.sum() / torch.numel(overshoot_penalty)\n",
    "\n",
    "        return mean_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(train_dataloader, val_dataloader, enc, dec, lr=2e-5):\n",
    "    r\"\"\"\n",
    "    Train encoder and decoder networks with `latent_dim` latent dimensions according\n",
    "    to the autoencoder objective (i.e., MSE reconstruction).\n",
    "\n",
    "    Returns the trained encoder and decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    optim = torch.optim.Adam(list(enc.parameters()) + list(dec.parameters()), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=5, gamma=0.5)  # Learning rate scheduler\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    counter = 0\n",
    "    clip_norm = 0.5\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=f\"{num_epochs} epochs total\"):\n",
    "        enc.train()\n",
    "        dec.train()\n",
    "        for raw_batch, preprocess_batch in tqdm(train_dataloader, desc=\"Training Batches\", leave=False):\n",
    "            optim.zero_grad()\n",
    "\n",
    "            loss_func = CustomLoss(2.25)\n",
    "\n",
    "            embed = enc(raw_batch)\n",
    "            output = dec(embed)\n",
    "\n",
    "            loss = loss_func(output, preprocess_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            torch_utils.clip_grad_norm_(list(enc.parameters()) + list(dec.parameters()), clip_norm)\n",
    "            optim.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        enc.eval()\n",
    "        dec.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for raw_batch_val, preprocess_batch_val in val_dataloader:\n",
    "                embed_val = enc(raw_batch_val)\n",
    "                output_val = dec(embed_val)\n",
    "                val_loss += loss_func(output_val, preprocess_batch_val).item()\n",
    "        val_loss /= len(val_dataloader)\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"[Autoencoder] epoch {epoch + 1: 4d}   Training Loss = {loss.item():.4g}   Validation Loss = {val_loss:.4g}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping triggered after {counter} epochs without improvement.\")\n",
    "                break\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 56\n",
    "\n",
    "num_heads = 4\n",
    "num_attn_layers = 2\n",
    "\n",
    "num_linear = 2\n",
    "lin_dim = 42\n",
    "latent_dim = 35\n",
    "\n",
    "output_size = 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "enc = Encoder(input_size, num_heads, num_attn_layers, num_linear, lin_dim, latent_dim).to(device)\n",
    "dec = Decoder(latent_dim, output_size, inner_layers=1).to(device)\n",
    "train_losses, val_losses = train_autoencoder(train_dataloader, val_dataloader, enc, dec)\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
